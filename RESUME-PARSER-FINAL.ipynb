{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class style:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sys\n",
    "import json\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from dateutil import relativedelta\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.matcher import Matcher\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import datefinder\n",
    "from datetime import date\n",
    "from segmentation import *  \n",
    "import csv\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import unicodedata\n",
    "from pdf2image import convert_from_path\n",
    "from os import path\n",
    "import subprocess\n",
    "\n",
    "from db import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MONTHS_SHORT = r'''(jan)|(feb)|(mar)|(apr)|(may)|(jun)|(jul)\n",
    "                   |(aug)|(sep)|(oct)|(nov)|(dec)'''\n",
    "MONTHS_LONG = r'''(january)|(february)|(march)|(april)|(may)|(june)|(july)|\n",
    "                   (august)|(september)|(october)|(november)|(december)'''\n",
    "MONTH = r'(' + MONTHS_SHORT + r'|' + MONTHS_LONG + r')'\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_img(file):\n",
    "    c = 0\n",
    "    \n",
    "    \n",
    "    pages = convert_from_path('resume/'+file+'.pdf',500, poppler_path=r'C:\\Program Files\\poppler-0.68.0\\bin')\n",
    "   \n",
    "    for page in pages:\n",
    "        c+=1\n",
    "        if not os.path.isdir('imgs/'+file):\n",
    "            os.mkdir('imgs/' + file)\n",
    "        page.save(os.path.join('imgs', file, str(c)+'.jpg'), 'JPEG')\n",
    "        \n",
    "def extract_using_tesseract(file):\n",
    "\n",
    "    #convert the PDF to image\n",
    "    if file.endswith('.pdf'):\n",
    "        filename = os.path.splitext(file)[0]\n",
    "        pdf_img(filename)\n",
    "    #convert the images to text\n",
    "    for img in os.listdir(os.path.join('imgs', str(filename))):\n",
    "        if(img.endswith('.jpg')):        \n",
    "            os.system('tesseract --dpi 500 ' + os.path.join('imgs', str(filename), img) + ' ' + os.path.join('imgs', str(filename), os.path.splitext(img)[0]))\n",
    "    #append text to python variable       \n",
    "    text = ''\n",
    "    img_text_files = os.listdir(os.path.join('imgs', filename))\n",
    "    for text_file in sorted(img_text_files):\n",
    "        if text_file.endswith('.txt'):\n",
    "            text_file_obj = open(os.path.join('imgs', filename, text_file), encoding=\"utf8\")\n",
    "            text = text +' '+ text_file_obj.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mobile_number(text, custom_regex=None):\n",
    "    number = '0000'\n",
    "    if not custom_regex:\n",
    "        mob_num_regex = r'''(\\+91)?(-)?\\s*?(91)?\\s*?(\\d{3})-?\\s*?(\\d{3})-?\\s*?(\\d{4})'''\n",
    "        phone = re.findall(re.compile(mob_num_regex), text)\n",
    "    else:\n",
    "        phone = re.findall(re.compile(custom_regex), text)\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        #print(\"Phone Number:\",number)\n",
    "    return number\n",
    "\n",
    "def extract_email(text):\n",
    "       \n",
    "        text = str(unicodedata.normalize('NFKD', text).encode('utf-8'))\n",
    "        index = text.find(\"@\")\n",
    "        if index > 0:\n",
    "            email = re.search(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "            if email:\n",
    "                return email.group(0)\n",
    "        return 'no val'\n",
    "    \n",
    "def remove_spl_characters(text):\n",
    "    text_alpha = ''\n",
    "    for i in text:\n",
    "        if not i.isalpha():\n",
    "            if i == '+':\n",
    "                text_alpha+=i\n",
    "            else:\n",
    "                text_alpha+=' '\n",
    "\n",
    "        else:\n",
    "            text_alpha+=i\n",
    "    return text_alpha\n",
    "\n",
    "def extract_skills(text):\n",
    "    text = remove_spl_characters(text)\n",
    "    doc=nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    data = pd.read_csv(\"csvfiles/skills.csv\")\n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.lower().strip() in skills:\n",
    "            skillset.append(token.lower())\n",
    "            \n",
    "    for token in doc.noun_chunks:\n",
    "        token= token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    skillset = list(dict.fromkeys(skillset))\n",
    "    if skillset == []:\n",
    "        skillset = ['no val']\n",
    "    #print(\"Skillset:\",[i.upper() for i in set([i.lower() for i in skillset])]) \n",
    "    return skillset\n",
    "\n",
    "def list_to_str(listt):\n",
    "    if listt == []:\n",
    "\n",
    "        strr = 'no val'\n",
    "    else:\n",
    "        strr = \"\"        \n",
    "        for x in listt:\n",
    "            if len(listt)>1:\n",
    "                strr += str(x)+\" \"\n",
    "            else:\n",
    "                strr = str(x)\n",
    "    return strr\n",
    "\n",
    "\n",
    "def lists_to_str(lists):\n",
    "    return list_to_str(lists[0]), list_to_str(lists[1])\n",
    "\n",
    "def find_degree_dates(text):\n",
    "    while(True):\n",
    "        index = text.find(\"GPA\")\n",
    "        if index >= 0:\n",
    "            temp = text[index:index+10]\n",
    "            text = text.replace(temp,\"\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        index = text.find(\"CGPA\")   \n",
    "        if index >= 0:\n",
    "            temp = text[index:index+10]\n",
    "            text = text.replace(temp,\"\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    matches = list(datefinder.find_dates(text))\n",
    "    edu_date = \"\"\n",
    "    if not matches == []:\n",
    "        currentYear = datetime.now().year\n",
    "        for temp_date in matches:\n",
    "            if not temp_date.year > currentYear + 4:\n",
    "                edu_date += str(temp_date.month) + \"/\" + str(temp_date.year) + \" - \"\n",
    "        year = edu_date[:(len(edu_date) - 2)]\n",
    "        return year\n",
    "    return 'no dates'\n",
    "        \n",
    "\n",
    "def extract_education(text):\n",
    "    \n",
    "    a=[]\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace(' ', '')\n",
    "    text = text.lower()\n",
    "    \n",
    "    def lower_list_elements(df_list):\n",
    "        df_l = []\n",
    "        for element in df_list:\n",
    "            df_l.append(\"\".join(element).replace(\" \", \"\").lower())\n",
    "        return df_l\n",
    "   \n",
    "    degree_df = pd.read_csv('csvfiles/qualification_degree_list.csv')\n",
    "    degree_list_abbrev = lower_list_elements([ el.lower() for el in sorted(list(degree_df['Abbrev']))])\n",
    "    degree_list_full = lower_list_elements([ el.lower() for el in sorted(list(degree_df['Full']))])\n",
    "    \n",
    "    major_df = pd.read_csv('csvfiles/educational_major.csv')\n",
    "    major_list = lower_list_elements([ el.lower() for el in sorted(list(major_df['Major']))])\n",
    "    \n",
    "    pos1, names1 = search_list_element_in_text(text, degree_list_abbrev)\n",
    "    pos2, names2 = search_list_element_in_text(text, degree_list_full)\n",
    "    pos3, names3 = search_list_element_in_text(text, major_list)\n",
    "    if not names1  == []:\n",
    "        names1 = remove_repeated_names_from_list(names1)\n",
    "        a.extend(names1)\n",
    "    if not names2 == []:\n",
    "        names2 = remove_repeated_names_from_list(names2)\n",
    "        a.extend(names2)\n",
    "    if not names3 == []:\n",
    "        names3 = remove_repeated_names_from_list(names3)\n",
    "        a.extend(names3)\n",
    "        \n",
    "#     doc= nlp(text)\n",
    "#     for tokens in doc:\n",
    "#         if tokens.text not in STOPWORDS:\n",
    "#             if tokens.text.lower() in degree_list_abbrev:\n",
    "#                 a.append(tokens.text.lower())\n",
    "#             if tokens.text.lower() in degree_list_full:\n",
    "#                  a.append(tokens.text.lower())\n",
    "#             if tokens.text.lower() in major_list:\n",
    "#                 a.append(tokens.text.lower())\n",
    "            \n",
    "#     if not a:\n",
    "#         a = extract_degree(text)\n",
    "    \n",
    "    d = re.findall('(\\d{4}-\\d{4})', text)\n",
    "    if not d:\n",
    "        d = find_degree_dates(text)\n",
    "    \n",
    "    return [a, d]\n",
    "\n",
    "\n",
    "def extract_degree(text):\n",
    "    \n",
    "    def get_education_word_list(dir_path):\n",
    "        file_name = \"education_segment.csv\"\n",
    "        reader = read_csv(dir_path+file_name)\n",
    "        education_word_list = []\n",
    "        for row in reader:\n",
    "            education_word_list.append(row[0])\n",
    "        return education_word_list \n",
    "    \n",
    "    def get_keywords(file_name):\n",
    "        dir_path = ''\n",
    "        reader = read_csv(dir_path + file_name)\n",
    "        keywords = []\n",
    "        for row in reader:\n",
    "            keywords.append(row[0])\n",
    "        return keywords\n",
    "\n",
    "    def read_csv(input_file):\n",
    "        file = open(input_file, 'r')\n",
    "        reader = csv.reader(file)\n",
    "        return reader\n",
    "\n",
    "    def get_major_word_list(dir_path):\n",
    "        file_name = \"csvfiles/educational_major.csv\"\n",
    "        reader = read_csv(dir_path + file_name)\n",
    "        major_list = []\n",
    "        for row in reader:\n",
    "            major_list.append(row[0])\n",
    "        return major_list\n",
    "\n",
    "    def search_major(text, degree, edu_obj): \n",
    "        maxim = -1\n",
    "        # Search for major\n",
    "        for major in major_word_list:\n",
    "            if major.lower() in (text.lower()) and len(major) > max:\n",
    "                major = str(major).title()\n",
    "                degree = degree.title()\n",
    "                maxim = len(str(major))\n",
    "        return degree_flag\n",
    "\n",
    "    def get_qualification_word_list(dir_path):\n",
    "        file_name = \"csvfiles/qualification_degree_list.csv\"\n",
    "        reader = read_csv(dir_path+file_name)\n",
    "        qualification_word_dict = {}\n",
    "        qualification_word_dict_no_spaces = {}\n",
    "        abbr_list = []\n",
    "        degree_list = []\n",
    "        for row in reader:\n",
    "            abbr_list.append(row[0])\n",
    "            abbr_list.append(row[0].replace(\" \", ''))\n",
    "            degree_list.append(row[1])\n",
    "            degree_list.append(row[0].replace(\" \", ''))\n",
    "            qualification_word_dict[row[0]] = row[1]\n",
    "            qualification_word_dict_no_spaces[row[0].replace(\" \", '')] = row[1].replace(\" \", '')\n",
    "        return qualification_word_dict, qualification_word_dict_no_spaces, abbr_list, degree_list\n",
    "\n",
    "    qualification_word_dict, qualification_word_dict_no_spaces, abbr_list, degree_list = get_qualification_word_list('')\n",
    "    major_word_list = get_major_word_list('')\n",
    "    education_degree_category = get_keywords(\"csvfiles/degree_category.csv\")\n",
    "    \n",
    "    max_len = 0\n",
    "    degree = 'no val'\n",
    "    major = 'no val'\n",
    "    for abbr, val in qualification_word_dict.items():\n",
    "        score1 = fuzz.ratio(str(abbr).lower(), text.lower())\n",
    "        score2 = fuzz.ratio(str(val).lower(), text.lower())\n",
    "        if score1 > 90 or score2 > 90 and max_len < len(val):\n",
    "            max_len = len(val)\n",
    "            degree = val\n",
    "\n",
    "        if str(abbr).lower() in text.lower():\n",
    "            major = val\n",
    "            \n",
    "\n",
    "    for word in major_word_list:\n",
    "        score3 = fuzz.partial_ratio(str(word).lower(), text.lower())\n",
    "        if score3 > 90 and max_len < len(word):\n",
    "            max_len = len(word)\n",
    "            major = word\n",
    "            \n",
    "    for abbr, val in qualification_word_dict_no_spaces.items():\n",
    "        score4 = fuzz.partial_ratio(str(abbr).lower(), text.lower())\n",
    "        score5 = fuzz.partial_ratio(str(val).lower(), text.lower())\n",
    "        if score4 > 90 or score5 > 90 and max_len < len(val):\n",
    "            max_len = len(val)\n",
    "            degree = val\n",
    "    return degree, major\n",
    "\n",
    "def extract_region(text):\n",
    "    doc=nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    region_df = pd.read_csv(\"csvfiles/region.csv\")\n",
    "    cities, states = list(region_df.loc[:, 'city']), list(region_df.loc[:, 'state'])\n",
    "    cities, states = [x.lower() for x in cities], [x.lower() for x in states]\n",
    "    city, state = 'no val', 'no val'\n",
    "\n",
    "    for token in tokens:\n",
    "        #for cities\n",
    "        if token.lower().strip() in cities: \n",
    "            city = token.lower()\n",
    "            i = cities.index(city)\n",
    "            state = states[i]\n",
    "        \n",
    "    return city, state\n",
    "\n",
    "\n",
    "def preprocess_collegename_files(file):\n",
    "    df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n",
    "    df.drop(df.iloc[:,1:], inplace = True, axis = 1)\n",
    "    if(file.find('college')!=-1):\n",
    "        df.rename(columns={'Name of the College':'collegename'},inplace=True)\n",
    "        df['collegename'] = df.collegename.str.split(\",\",expand=True) \n",
    "    else:\n",
    "        df.rename(columns={'Name of the University':'univname'},inplace=True)\n",
    "        df['univname'] = df.univname.str.split(\",\",expand=True)\n",
    "        \n",
    "    df_list = df.values\n",
    "    df_l = []\n",
    "    for element in df_list:\n",
    "        df_l.append(\"\".join(element).replace(\" \", \"\").lower())\n",
    "    return df_l\n",
    "\n",
    "\n",
    "def search_list_element_in_text(text, l):\n",
    "    pos = []\n",
    "    names = []\n",
    "    for val in l:\n",
    "        x = text.find(val)\n",
    "        if(x!=-1):\n",
    "            pos.append(x)\n",
    "            names.append(val)\n",
    "    return pos, names\n",
    "\n",
    "\n",
    "def extract_college_or_uni(text, college_l, uni_l):\n",
    "    colleges = 'no val'\n",
    "    pos, names = search_list_element_in_text(text, college_l)\n",
    "    if(len(pos) == 0):\n",
    "        pos, names = search_list_element_in_text(text, uni_l)\n",
    "    pos = list(dict.fromkeys(pos))\n",
    "    names = list(dict.fromkeys(names))\n",
    "    colleges = remove_repeated_names_from_list(names)\n",
    "    return colleges\n",
    "\n",
    "\n",
    "def remove_repeated_names_from_list(names):\n",
    "    multiple = []\n",
    "    for i in range(len(names)):\n",
    "        name = names[i]\n",
    "        for j in range(len(names)):\n",
    "            if names[j].find(name)!= -1:\n",
    "                name = names[j]\n",
    "        multiple.append(name)\n",
    "    multiple = list(dict.fromkeys(multiple))\n",
    "    return multiple\n",
    "\n",
    "\n",
    "def extract_college(text):\n",
    "    college_l = list(dict.fromkeys(preprocess_collegename_files('csvfiles/college.csv')))\n",
    "    uni_l = list(dict.fromkeys(preprocess_collegename_files('csvfiles/unis.csv')))\n",
    "    text = text.replace(\" \",\"\")\n",
    "    text = text.lower()\n",
    "    name = extract_college_or_uni(text, college_l, uni_l)\n",
    "    if name == []:\n",
    "        name = ['no val']\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces_to_dates(date):\n",
    "    match = re.match(r\"([a-z]+)([0-9]+)\", date, re.I)\n",
    "    \n",
    "    if match:\n",
    "        items = match.groups()\n",
    "        date = ''.join(items[0] + ' ' + items[1])\n",
    "        return date\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#no of months for each set of dates\n",
    "def get_number_of_months_from_dates(date1, date2):\n",
    "    date1 = add_spaces_to_dates(date1)\n",
    "    date2 = add_spaces_to_dates(date2)\n",
    "\n",
    "    if date1 and date2:\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%b %Y')\n",
    "            \n",
    "        try:\n",
    "            if len(date1.split()[0]) > 3:\n",
    "                date1 = date1.split()\n",
    "                date1 = date1[0][:3] + ' ' + date1[1]\n",
    "\n",
    "            if len(date2.split()[0]) > 3:\n",
    "                date2 = date2.split()\n",
    "                date2 = date2[0][:3] + ' ' + date2[1]\n",
    "        except IndexError: \n",
    "            return 0\n",
    "        try:\n",
    "            date1 = datetime.strptime(str(date1), '%b %Y')\n",
    "            date2 = datetime.strptime(str(date2), '%b %Y')\n",
    "            months_of_experience = relativedelta.relativedelta(date2, date1)\n",
    "            months_of_experience = (months_of_experience.years\n",
    "                                    * 12 + months_of_experience.months)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        return months_of_experience\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#extract all dates from resume\n",
    "def find_dates(text):\n",
    "    date_list = re.findall('(\\w+.\\d+)\\s*(\\D|to)\\s*(\\w+.\\d+|present|Present)', text)\n",
    "    str_list = []\n",
    "    for tupl in date_list:\n",
    "        name = tupl[0] + \" \"+ tupl[2]\n",
    "        str_list.append(\"\".join(name))\n",
    "    return str_list\n",
    "\n",
    "def get_work_exp_mmyyyy(text):\n",
    "    while(True):\n",
    "        index = text.find(\"GPA\")\n",
    "        if index >= 0:\n",
    "            temp = text[index:index+10]\n",
    "            text = text.replace(temp,\"\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        index = text.find(\"CGPA\")   \n",
    "        if index >= 0:\n",
    "            temp = text[index:index+10]\n",
    "            text = text.replace(temp,\"\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    matches = list(datefinder.find_dates(text))\n",
    "    edu_date = \"\"\n",
    "    \n",
    "    def elapsed_months(date1, date2=datetime.today()):\n",
    "        return abs(date1.year * 12 + date1.month) - (date2.year * 12 + date2.month)\n",
    "    \n",
    "    if not matches == []:\n",
    "        currentYear = datetime.now().year\n",
    "        for temp_date in matches:\n",
    "            \n",
    "            if not temp_date.year > currentYear + 4:\n",
    "                edu_date += str(temp_date.month) + \"/\" + str(temp_date.year) + \" - \"\n",
    "        year = edu_date[:(len(edu_date) - 2)]\n",
    "        if 'present' in text.lower():\n",
    "            year = year + date.today().strftime(\"%m/%y\")\n",
    "            matches.append(datetime.now())\n",
    "        return elapsed_months(max(matches), min(matches))\n",
    "    else: return 0\n",
    "\n",
    "#from extracted dates, calculate sum of each work experience\n",
    "def get_total_work_experience(text):\n",
    "    resume_dates = find_dates(text)\n",
    "    exp_ = []\n",
    "    total_exp = 0\n",
    "    for date in resume_dates:\n",
    "        experience = re.search(MONTH, date.lower())\n",
    "        if experience:\n",
    "            for dates in date.split():\n",
    "                \n",
    "                is_date = re.search(MONTH, dates.lower())\n",
    "                if 'present' in dates.lower():\n",
    "                    is_date = 0\n",
    "                \n",
    "                if is_date:\n",
    "                    exp_.append(date.split())\n",
    "                \n",
    "    total_exp = sum(\n",
    "        [get_number_of_months_from_dates(i[0], i[1]) for i in exp_]\n",
    "    )\n",
    "    if total_exp == 0:\n",
    "        total_exp = get_work_exp_mmyyyy(text)\n",
    "    if total_exp < 12:\n",
    "        total_experience = str(total_exp) + \" months\"\n",
    "    else:\n",
    "        year = str(int(total_exp/12)) + ' years'\n",
    "        mon = str(int((total_exp/12 % 1)*12)) + ' months'\n",
    "        total_experience = year + ' ' + mon\n",
    "    return total_experience\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rec(dic):\n",
    "    keys = dic.keys()\n",
    "    for key in keys:\n",
    "        print(style.BOLD + key + ': '+ style.END)\n",
    "        print(dic[key])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text):\n",
    "    result = {\n",
    "                'email': 'No val',\n",
    "                'mobile': '0000',\n",
    "                'skills': 'No val',\n",
    "                'education': 'No val',\n",
    "                'years': 'No val',\n",
    "                'city': 'No val',\n",
    "                'state': 'No val',\n",
    "                'work_experience': 'No val',\n",
    "                'college': 'No val'\n",
    "            }\n",
    "    email=str(extract_email(text)).lower()\n",
    "    mobile_number=str(extract_mobile_number(text))\n",
    "    skills=extract_skills(text)\n",
    "    a, e = extract_education(text)\n",
    "    if a and e:\n",
    "        education, graduation_years=lists_to_str([a, e])\n",
    "    else:\n",
    "        education, graduation_years= 'no val', 'no val'\n",
    "    city, state= extract_region(text)\n",
    "    exp = get_total_work_experience(text)\n",
    "    college_name = extract_college(text)\n",
    "\n",
    "    result = {\n",
    "                'email': email,\n",
    "                'mobile': mobile_number,\n",
    "                'skills': skills,\n",
    "                'education': education,\n",
    "                'years': graduation_years,\n",
    "                'city': city,\n",
    "                'state': state,\n",
    "                'work_experience': exp,\n",
    "                'college': college_name\n",
    "            }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(text):\n",
    "    record = {\n",
    "                'email': 'no val',\n",
    "                'mobile': '0000',\n",
    "                'skills': 'No val',\n",
    "                'education': 'No val',\n",
    "                'years': 'No val',\n",
    "                'city': 'No val',\n",
    "                'state': 'No val',\n",
    "                'work_experience': 'No val',\n",
    "                'college': 'No val'\n",
    "             }\n",
    "    text2 = text.replace('\\n', '\\n ')\n",
    "    text = text.replace(' ', '')\n",
    "    \n",
    "    text = text + \"$$$\"\n",
    "    text2 = text2 + \"$$$\"\n",
    "    segment_category, segment_text, segment_count = seg(text)\n",
    "    segment_category2, segment_text2, segment_count2 = seg(text2)\n",
    "    \n",
    "    if segment_count <=1 or segment_count2 <=1:\n",
    "        record = parse(text)\n",
    "    else: \n",
    "        if 'education_segment' in segment_category2:\n",
    "            record['education'], record['years'] = lists_to_str(extract_education(segment_text2[segment_category2.index('education_segment')]))\n",
    "            record['college'] = extract_college(segment_text2[segment_category2.index('education_segment')])\n",
    "        elif 'education_segment' in segment_category:\n",
    "            record['education'], record['years'] = lists_to_str(extract_education(segment_text[segment_category.index('education_segment')]))\n",
    "            record['college'] = extract_college(segment_text[segment_category.index('education_segment')])\n",
    "        else:\n",
    "            record['college'] = extract_college(text2)\n",
    "            record['education'], record['years'] = lists_to_str(extract_education(text2))\n",
    "            \n",
    "        if 'personaldetails_segment' in segment_category:\n",
    "        \n",
    "            record['mobile'] = extract_mobile_number(segment_text2[segment_category2.index('personaldetails_segment')])\n",
    "            record['email'] = extract_email(segment_text2[segment_category2.index('personaldetails_segment')])\n",
    "            if record['mobile'] == None:\n",
    "                record['mobile'] = extract_mobile_number(text2)\n",
    "            if record['email'] == None: \n",
    "                record['email'] = extract_email(text2)\n",
    "            record['city'], record['state'] = extract_region(segment_text2[segment_category2.index('personaldetails_segment')])\n",
    "            if record['city'] == None or record['state'] == None:\n",
    "                record['city'], record['state'] = extract_region(text2)         \n",
    "        else:      \n",
    "            \n",
    "            record['mobile'] = extract_mobile_number(text2)\n",
    "            record['email'] = extract_email(text2)\n",
    "            record['city'], record['state'] = extract_region(text2)  \n",
    "            \n",
    "        \n",
    "        if 'skill_segment' in segment_category:\n",
    "            record['skills'] = extract_skills(segment_text2[segment_category2.index('skill_segment')])\n",
    "        else:\n",
    "            record['skills'] = extract_skills(text2)\n",
    "        \n",
    "        if 'work_experience_segment' in segment_category:\n",
    "            record['work_experience'] = get_total_work_experience(segment_text2[segment_category2.index('work_experience_segment')])\n",
    "        else:\n",
    "            record['work_experience'] = get_total_work_experience(text)\n",
    "       \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_segments(segment_category, segment_text, segment_count):\n",
    "    for i in range(segment_count):\n",
    "        print(style.BOLD + 'category: ' + segment_category[i] + style.END)\n",
    "        print('segment: ' + segment_text[i])\n",
    "        print('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segprnt(text):\n",
    "    text = text + \"$$$\"\n",
    "    segment_category, segment_text, segment_count = seg(text)\n",
    "    print_segments(segment_category, segment_text, segment_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!tesseract --dpi 300 'imgs/15808874656NehaJoseph._Resume/.jpg' 'imgs/15808874656NehaJoseph._Resume/3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = extract_using_tesseract('VarunVijayResume.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_category, segment_text, segment_count = seg(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_segments(segment_category, segment_text, segment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = extract_using_tesseract('HariniAshokresume.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Harini Ashok (+91) 7010808133 | aharini2008@gmail.com\\n\\nINTERNSHIP EXPERIENCE\\n\\n \\n\\nNVIDIA Graphics Pvt Ltd\\nDeep Learning Research Intern , Jan 2020 —- May 2020\\n\\n¢ Worked on research in Generative Neural Networks with a focus on Image Reconstruction.\\n¢ Built two Image inpainting models using GANs, using Tensorflow and PyTorch.\\n\\nDelium Technologies\\nComputer Scientist Intern, May 2019 - July 2019\\n\\n¢ Worked on the core of Customer Insights — module of the company’s retail automation product “The Eye”.\\n\\n¢ Performed Data Analysis on Retail datasets to find correlation between a large number of products, and found links between customer\\nchurn and specific product availability along with customer buying patterns.\\n\\nInfotech Software Dealers Association\\nEvent Management Intern , November 2018 - January 2019\\n\\n¢ Interned as an Event Organizer for the international event - TechSummit 9 organized by ISODA.\\n¢ — Responsible for handling logistics, co-ordinating sponsors and speakers for the event.\\n\\nFuturenet Technologies India Pvt Ltd.\\nData Analyst Intern, November 2017 - January 2018\\n\\n¢ Worked on visualization and prediction for Key Performance Indicators in sales.\\n¢ Analysed company’s yearly sales data to find lead contributor to profits in various aspects.\\n\\nPROJECTS\\n\\n \\n\\nResume Parser\\n¢ Created a tool for analyzing resumes, that extracts the desired information and and inserts the information into a database.\\n¢ Built a UI to facilitate browsing parsed resumes and uploading new resumes dynamically.\\n\\nImage Inpainting\\n¢ Used Generative Adversarial Neural Networks to perform Semantic Image Inpainting.\\n¢ Built an Inpainting model using DCGANSs and one using Deep Image Prior.\\n\\nCustomer Segmentation for Retail Automation\\n¢ Created a Python based script to perform Customer Segmentation using Recency, Frequency and Monetary Analysis.\\n\\nAutomation of detection of choking in Air Filters\\n¢ Smart India Hackathon 2019 finalist, Designed a working prototype using Python and Raspberry Pi to automate choking detection.\\n\\n¢ Created a website to notify choking percentage and visualize it over a time period. The prototype notifies by E-mail and text message,\\nwhenever air filter is about to be choked.\\n\\nReal time Object Detection with Voice Feedback\\n¢ Created a Deep Learning model using YOLO (You-Only-Look-Once) algorithm that predicts the position of real time images fed through a\\nwebcam. It translates the object coordinates into voice feedback.\\n\\nEDUCATION\\n\\n \\n\\nSri Venkateswara College of Engineering\\nBachelor's of Engineering in Computer Science and Engineering, May 2020\\n\\n¢ = CGPA: 9.25\\n* 2019-2020 Secretary | ACM Student Chapter\\nO Co-headed a team of 40 and organised technical symposium and department level events. Organized a 24 hour, inter-college college Hackathon. Raised\\nfunds over 50,000 INR.\\nO Volunteered in organizing Webinars for women in Tech, an inter-college coding run last winter, seminars and guest lectures inside the college, on various\\ntopics.\\n\\n* 2018 — 2019 Secretary | Rotaract Club\\nO Part of the mentoring team for the Communication and English skills program designed to help students with non-English primary course of instruction.\\n\\nSKILLS & PROGRAMMING LANGUAGES\\n\\n \\n\\n¢ Programming Languages: Python, R, C/C++, HTML/CSS, MySQL, Javascript\\n¢ Packages: Tensorflow, PyTorch, Scikit Learn, Numpy\\n¢ Web-frameworks: Django, Flask\\n\\x0c Harini Ashok (+91) 7010808133 | aharini2008@gmail.com\\n\\nINTERNSHIP EXPERIENCE\\n\\nNVIDIA Graphics Pvt Ltd\\n\\nDeep Learning Research Intern , Jan 2020 - May 2020\\n\\n* Worked on research in Generative Neural Networks with a focus on Image Reconstruction.\\n* Built two Image inpainting models using GANs, using Tensorflow and PyTorch.\\n\\n \\n\\nDelium Technologies\\nComputer Scientist Intern, May 2019 - July 2019\\n* Worked on the core of Customer Insights —- module of the company’s retail automation product “The Eye”.\\n\\n*¢ Performed Data Analysis on Retail datasets to find correlation between a large number of products, and found links between customer\\nchurn and specific product availability along with customer buying patterns.\\n\\nInfotech Software Dealers Association\\n\\nEvent Management Intern , November 2018 - January 2019\\n\\n*  Interned as an Event Organizer for the international event - TechSummit 9 organized by ISODA.\\n* Responsible for handling logistics, co-ordinating sponsors and speakers for the event.\\n\\nFuturenet Technologies India Pvt Ltd.\\nData Analyst Intern, November 2017 - January 2018\\n\\n¢ Worked on visualization and prediction for Key Performance Indicators in sales.\\n¢ Analysed company’s yearly sales data to find lead contributor to profits in various aspects.\\n\\nPROJECTS\\n\\n \\n\\nResume Parser\\n* Created a tool for analyzing resumes, that extracts the desired information and and inserts the information into a database.\\n* Built a UI to facilitate browsing parsed resumes and uploading new resumes dynamically.\\n\\nImage Inpainting\\n* Used Generative Adversarial Neural Networks to perform Semantic Image Inpainting.\\n* Built an Inpainting model using DCGANs and one using Deep Image Prior.\\n\\nCustomer Segmentation for Retail Automation\\n* Created a Python based script to perform Customer Segmentation using Recency, Frequency and Monetary Analysis.\\n\\nAutomation of detection of choking in Air Filters\\n* Smart India Hackathon 2019 finalist, Designed a working prototype using Python and Raspberry Pi to automate choking detection.\\n\\n* Created a website to notify choking percentage and visualize it over a time period. The prototype notifies by E-mail and text message,\\nwhenever air filter is about to be choked.\\n\\nReal time Object Detection with Voice Feedback\\n* Created a Deep Learning model using YOLO (You-Only-Look-Once) algorithm that predicts the position of real time images fed through a\\nwebcam. It translates the object coordinates into voice feedback.\\n\\nEDUCATION\\n\\n \\n\\nSri Venkateswara College of Engineering\\nBachelor's of Engineering in Computer Science and Engineering, May 2020\\n*  CGPA: 9.25\\n\\n# 2019-2020 Secretary | ACM Student Chapter\\n\\n° Co-headed a team of 40 and organised technical symposium and department level events. Organized a 24 hour, inter-college college Hackathon. Raised\\nfunds over 50,000 INR.\\n\\n° Volunteered in organizing Webinars for women in Tech, an inter-college coding run last winter, seminars and guest lectures inside the college, on various\\ntopics.\\n\\n¢ 2018 — 2019 Secretary | Rotaract Club\\nart of the mentoring team for the Communication and English skills program designed to help students with non-English primary course of instruction.\\n\\nSKILLS & PROGRAMMING LANGUAGES\\n\\n \\n\\n* Programming Languages: Python, R, C/C++, HTML/CSS, MySQL, Javascript\\n* Packages: Tensorflow, PyTorch, Scikit Learn, Numpy\\n* Web-frameworks: Django, Flask\\n\\x0c\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': 'aharini2008@gmail.com',\n",
       " 'mobile': '7010808133',\n",
       " 'skills': ['programming',\n",
       "  'python',\n",
       "  'r',\n",
       "  'c',\n",
       "  'c++',\n",
       "  'html',\n",
       "  'css',\n",
       "  'mysql',\n",
       "  'javascript',\n",
       "  'tensorflow',\n",
       "  'pytorch',\n",
       "  'numpy',\n",
       "  'django',\n",
       "  'flask',\n",
       "  'gmail',\n",
       "  'research',\n",
       "  'retail',\n",
       "  'automation',\n",
       "  'analysis',\n",
       "  'datasets',\n",
       "  'international',\n",
       "  'logistics',\n",
       "  'sales',\n",
       "  'parser',\n",
       "  'database',\n",
       "  'ui',\n",
       "  'segmentation',\n",
       "  'prototype',\n",
       "  'website',\n",
       "  'engineering',\n",
       "  'technical',\n",
       "  'coding',\n",
       "  'communication',\n",
       "  'english',\n",
       "  'key performance indicators',\n",
       "  'computer science'],\n",
       " 'education': 'art communication computerscience engineering english ',\n",
       " 'years': '2019-2020',\n",
       " 'city': 'no val',\n",
       " 'state': 'no val',\n",
       " 'work_experience': '8 months',\n",
       " 'college': ['srivenkateswaracollegeofengineering']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "\n",
    "im = Image.open(\"imgs/HariniAshokresume/1.jpg\")\n",
    "\n",
    "text = pytesseract.image_to_string(im, lang = 'eng',config='-l eng --oem 1 --psm 3')\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
